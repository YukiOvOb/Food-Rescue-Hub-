name: Food Rescue Hub CI/CD

on:
  push:
    branches:
      - main
      - Sprint2/**
      - feature/**
  pull_request:
    branches: 
      - main
  workflow_dispatch:
    inputs:
      jmeter_target_host:
        description: "Optional. Host or domain for deployment load test (example: ec2-xx-xx-xx-xx.compute-1.amazonaws.com)"
        required: false
        type: string
      jmeter_target_port:
        description: "Optional. Port for deployment load test."
        required: false
        default: "8080"
        type: string
      jmeter_target_protocol:
        description: "Optional. Protocol for deployment load test."
        required: false
        default: "http"
        type: choice
        options:
          - http
          - https

concurrency:
  group: frh-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read

env:
  REPORT_ONLY: "true"          # true = don't fail pipeline on lint/scans; false = strict gate
  PYTHON_DIR: "ai_service"
  ANDROID_DIR: "android"

  # Local integration endpoints inside GitHub runner:
  PROD_URL: "http://13.228.183.177:80"   # nginx port for production
  BACKEND_LOCAL_URL: "http://localhost:8080"
  OPENAPI_PATH: "/v3/api-docs"   # Swagger/OpenAPI (Spring Boot default)
  SMOKE_PATH: "/api/hello"       # change to your real endpoint

jobs:
  # =========================================================
  # STAGE 0: Detect project parts (safe skipping)
  # =========================================================

  detect:
    name: Detect project folders
    runs-on: ubuntu-latest
    outputs:
      has_backend: ${{ steps.detect.outputs.has_backend }}
      has_frontend: ${{ steps.detect.outputs.has_frontend }}
      has_python_src: ${{ steps.detect.outputs.has_python_src }}
      has_android: ${{ steps.detect.outputs.has_android }}
      has_compose: ${{ steps.detect.outputs.has_compose }}
      has_playwright: ${{ steps.detect.outputs.has_playwright }}
    steps:
      - uses: actions/checkout@v4

      - id: detect
        shell: bash
        run: |
          set -e

          if [ -f "backend/pom.xml" ]; then echo "has_backend=true" >> "$GITHUB_OUTPUT"; else echo "has_backend=false" >> "$GITHUB_OUTPUT"; fi
          if [ -f "frontend/package.json" ]; then echo "has_frontend=true" >> "$GITHUB_OUTPUT"; else echo "has_frontend=false" >> "$GITHUB_OUTPUT"; fi

          if [ -d "${PYTHON_DIR}" ] && find "${PYTHON_DIR}" -type f -name "*.py" | grep -q .; then
            echo "has_python_src=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_python_src=false" >> "$GITHUB_OUTPUT"
          fi

          if [ -d "${ANDROID_DIR}" ] && [ -f "${ANDROID_DIR}/gradlew" ]; then echo "has_android=true" >> "$GITHUB_OUTPUT"; else echo "has_android=false" >> "$GITHUB_OUTPUT"; fi
          if [ -f "docker-compose.yml" ] || [ -f "compose.yml" ]; then echo "has_compose=true" >> "$GITHUB_OUTPUT"; else echo "has_compose=false" >> "$GITHUB_OUTPUT"; fi
          if [ -f "frontend/playwright.config.ts" ] || [ -f "frontend/playwright.config.js" ]; then echo "has_playwright=true" >> "$GITHUB_OUTPUT"; else echo "has_playwright=false" >> "$GITHUB_OUTPUT"; fi

      - name: Print detected flags
        run: |
          echo "has_backend=${{ steps.detect.outputs.has_backend }}"
          echo "has_frontend=${{ steps.detect.outputs.has_frontend }}"
          echo "has_python_src=${{ steps.detect.outputs.has_python_src }}"
          echo "has_android=${{ steps.detect.outputs.has_android }}"
          echo "has_compose=${{ steps.detect.outputs.has_compose }}"
          echo "has_playwright=${{ steps.detect.outputs.has_playwright }}"


  # =========================================================
  # STAGE 1: Unit Tests (Java) + JaCoCo
  # =========================================================
  backend-junit:
    name: Unit Tests (Java Backend) + JaCoCo
    runs-on: ubuntu-latest
    needs: [detect]
    steps:
      - uses: actions/checkout@v4

      - name: Skip (no backend)
        if: needs.detect.outputs.has_backend != 'true'
        run: echo "No backend/pom.xml found -> skipping Java tests."

      - name: Set up JDK 17
        if: needs.detect.outputs.has_backend == 'true'
        uses: actions/setup-java@v4
        with:
          java-version: "17"
          distribution: "temurin"
          cache: maven

      - name: Run unit tests + coverage (Maven)
        if: needs.detect.outputs.has_backend == 'true'
        working-directory: backend
        env:
          SPRING_PROFILES_ACTIVE: test
        shell: bash
        run: |
          set -e
          chmod +x mvnw
          ./mvnw -q clean verify


      - name: Upload Maven surefire reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: java-unit-test-reports
          path: backend/target/surefire-reports/**
          if-no-files-found: warn

      - name: Upload JaCoCo report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: jacoco-report
          path: backend/target/site/jacoco/**
          if-no-files-found: warn


  # =========================================================
  # STAGE 1: Unit Tests (Python)
  # =========================================================
  python-tests:
    name: Unit Tests (Python ML) + Coverage
    runs-on: ubuntu-latest
    needs: [detect]
    steps:
      - uses: actions/checkout@v4

      - name: Skip (no python)
        if: needs.detect.outputs.has_python_src != 'true'
        run: echo "No Python source detected -> skipping Python tests."

      - name: Set up Python
        if: needs.detect.outputs.has_python_src == 'true'
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install Python deps
        if: needs.detect.outputs.has_python_src == 'true'
        shell: bash
        run: |
          set -e
          cd "${PYTHON_DIR}"

          if [ -f "requirements.txt" ]; then
            pip install -r requirements.txt
          elif [ -f "pyproject.toml" ]; then
            pip install .
          else
            echo "No requirements.txt / pyproject.toml found -> installing only test tools"
          fi

          pip install pytest pytest-cov

      - name: Run pytest + coverage (if tests exist)
        if: needs.detect.outputs.has_python_src == 'true'
        shell: bash
        run: |
          set -e
          cd "${PYTHON_DIR}"

          if [ -d "tests" ] || ls -1 test* >/dev/null 2>&1; then
            pytest -q \
              --junitxml=pytest-junit.xml \
              --cov=. \
              --cov-report=xml:coverage.xml \
              --cov-report=html:htmlcov || \
              ( [ "${REPORT_ONLY}" = "true" ] && exit 0 || exit 1 )
          else
            echo "No Python tests found -> skipping pytest"
          fi

      - name: Upload pytest reports
        if: always() && needs.detect.outputs.has_python_src == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: python-test-reports
          path: |
            ${{ env.PYTHON_DIR }}/pytest-junit.xml
            ${{ env.PYTHON_DIR }}/coverage.xml
            ${{ env.PYTHON_DIR }}/htmlcov/**
          if-no-files-found: warn


  # =========================================================
  # STAGE 1: Unit Tests (Android) + JaCoCo
  # =========================================================
  android-unit-tests:
    name: Unit Tests (Android) + JaCoCo
    runs-on: ubuntu-latest
    needs: [detect]
    steps:
      - uses: actions/checkout@v4

      - name: Skip (no android)
        if: needs.detect.outputs.has_android != 'true'
        run: echo "No android/gradlew found -> skipping Android unit tests."

      - name: Set up JDK 17
        if: needs.detect.outputs.has_android == 'true'
        uses: actions/setup-java@v4
        with:
          java-version: "17"
          distribution: "temurin"
          cache: gradle

      - name: Android unit tests + coverage (devDebug)
        if: needs.detect.outputs.has_android == 'true'
        shell: bash
        run: |
          set -e
          cd "${ANDROID_DIR}"
          chmod +x gradlew
          ./gradlew :app:testDevDebugUnitTest :app:jacocoDevDebugUnitTestReport

      - name: Upload Android unit test results (JUnit XML)
        if: always() && needs.detect.outputs.has_android == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: android-unit-test-results
          path: |
            android/**/build/test-results/test*UnitTest/*.xml
            android/**/build/reports/tests/test*UnitTest/**
          if-no-files-found: warn

      - name: Upload Android JaCoCo report
        if: always() && needs.detect.outputs.has_android == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: android-jacoco-report
          path: |
            android/**/build/reports/jacoco/**
          if-no-files-found: warn


  # =========================================================
  # STAGE 1: Lint & Code Quality
  # =========================================================
  lint-code-quality:
    name: Lint & Code Quality (Java + Python + React + Thymeleaf + Android)
    runs-on: ubuntu-latest
    needs: [detect]
    steps:
      - uses: actions/checkout@v4

      # ---- Java Checkstyle ----
      - name: Set up JDK 17
        if: needs.detect.outputs.has_backend == 'true'
        uses: actions/setup-java@v4
        with:
          java-version: "17"
          distribution: "temurin"
          cache: maven

      - name: Java Checkstyle
        if: needs.detect.outputs.has_backend == 'true'
        working-directory: backend
        shell: bash
        run: |
          chmod +x mvnw
          if [ "${REPORT_ONLY}" = "true" ]; then
            ./mvnw -q checkstyle:check || true
          else
            ./mvnw -q checkstyle:check
          fi

      - name: Upload Checkstyle report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: backend-checkstyle-report
          path: |
            backend/target/checkstyle-result.xml
            backend/target/site/checkstyle.*
          if-no-files-found: warn

      # ---- Python Ruff ----
      - name: Set up Python (lint)
        if: needs.detect.outputs.has_python_src == 'true'
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Python Ruff lint # only if the python files are present
        if: needs.detect.outputs.has_python_src == 'true'
        shell: bash
        run: |
          set -e
          pip install ruff
          cd "${PYTHON_DIR}"
          if [ "${REPORT_ONLY}" = "true" ]; then
            ruff check . --output-format=full > ruff-report.txt || true
          else
            ruff check . --output-format=full > ruff-report.txt
          fi

      - name: Upload Ruff report
        if: always() && needs.detect.outputs.has_python_src == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: python-ruff-report
          path: ${{ env.PYTHON_DIR }}/ruff-report.txt
          if-no-files-found: warn

      # ---- React ESLint ----
      - name: Setup Node 20
        if: needs.detect.outputs.has_frontend == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: npm
          cache-dependency-path: frontend/package-lock.json

      - name: Frontend install deps
        if: needs.detect.outputs.has_frontend == 'true'
        working-directory: frontend
        run: npm ci

      - name: Frontend ESLint report
        if: needs.detect.outputs.has_frontend == 'true'
        working-directory: frontend
        shell: bash
        run: |
          npx eslint "src/**/*.{js,jsx}" -f json -o eslint-report.json || true

      - name: Upload ESLint report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: react-eslint-report
          path: frontend/eslint-report.json
          if-no-files-found: warn

      # ---- Thymeleaf HTMLHint ----
      - name: Install HTMLHint
        if: needs.detect.outputs.has_backend == 'true'
        run: npm i -g htmlhint

      - name: Thymeleaf HTMLHint
        if: needs.detect.outputs.has_backend == 'true'
        shell: bash
        run: |
          if [ "${REPORT_ONLY}" = "true" ]; then
            htmlhint "backend/src/main/resources/templates/**/*.html" > htmlhint-report.txt || true
          else
            htmlhint "backend/src/main/resources/templates/**/*.html" > htmlhint-report.txt
          fi

      - name: Upload HTMLHint report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: thymeleaf-htmlhint-report
          path: htmlhint-report.txt
          if-no-files-found: warn

      # ---- Android Lint ----
      - name: Android lint
        if: needs.detect.outputs.has_android == 'true'
        shell: bash
        run: |
          cd "${ANDROID_DIR}"
          chmod +x gradlew
          if [ "${REPORT_ONLY}" = "true" ]; then
            ./gradlew lint || true
          else
            ./gradlew lint
          fi

      - name: Upload Android lint reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: android-lint-report
          path: |
            android/**/build/reports/lint-results*.html
            android/**/build/reports/lint-results*.xml
          if-no-files-found: warn


  # =========================================================
  # STAGE 1: SAST (SonarCloud)
  # =========================================================
  sonarcloud:
    name: SonarCloud SAST
    runs-on: ubuntu-latest
    needs: [detect, backend-junit, python-tests, android-unit-tests, lint-code-quality]
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0   # REQUIRED

      - name: Download JaCoCo report artifact (for coverage in Sonar)
        if: needs.detect.outputs.has_backend == 'true'
        uses: actions/download-artifact@v4
        with:
          name: jacoco-report
          path: .artifacts/jacoco-report

      - name: Place JaCoCo XML where Sonar expects
        if: needs.detect.outputs.has_backend == 'true'
        shell: bash
        run: |
          set -e
          # The artifact contains backend/target/site/jacoco/**
          if [ -f ".artifacts/jacoco-report/backend/target/site/jacoco/jacoco.xml" ]; then
            mkdir -p backend/target/site/jacoco
            cp -r .artifacts/jacoco-report/backend/target/site/jacoco/* backend/target/site/jacoco/
          else
            echo "JaCoCo artifact missing jacoco.xml; Sonar coverage may be empty."
          fi

      - name: Download Python coverage artifact (for coverage in Sonar)
        if: needs.detect.outputs.has_python_src == 'true'
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: python-test-reports
          path: .artifacts/python-test-reports

      - name: Place Python coverage reports where Sonar expects
        if: needs.detect.outputs.has_python_src == 'true'
        shell: bash
        run: |
          set -e
          # The artifact contains ai_service/coverage.xml and ai_service/pytest-junit.xml
          if [ -f ".artifacts/python-test-reports/ai_service/coverage.xml" ]; then
            mkdir -p ai_service
            cp -f .artifacts/python-test-reports/ai_service/coverage.xml ai_service/coverage.xml
          fi
          if [ -f ".artifacts/python-test-reports/ai_service/pytest-junit.xml" ]; then
            mkdir -p ai_service
            cp -f .artifacts/python-test-reports/ai_service/pytest-junit.xml ai_service/pytest-junit.xml
          fi

      - name: Download Android JaCoCo report artifact (for coverage in Sonar)
        if: needs.detect.outputs.has_android == 'true'
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: android-jacoco-report
          path: .artifacts/android-jacoco-report

      - name: Place Android JaCoCo XML where Sonar expects
        if: needs.detect.outputs.has_android == 'true'
        shell: bash
        run: |
          set -e
          # Copy any jacoco XMLs back into workspace under android/ so sonar-project.properties can reference them.
          if ls .artifacts/android-jacoco-report/android/**/build/reports/jacoco/**/*.xml >/dev/null 2>&1; then
            mkdir -p android
            cp -r .artifacts/android-jacoco-report/android/* android/
          else
            echo "Android JaCoCo XML not found in artifact; Android coverage may be empty."
          fi

      - name: Set up JDK 17 (for build)
        uses: actions/setup-java@v4
        with:
          java-version: "17"
          distribution: "temurin"
          cache: maven

      - name: Build backend (compile classes for Sonar)
        working-directory: backend
        shell: bash
        run: |
          set -e
          chmod +x mvnw
          ./mvnw -q -DskipTests package

      - name: SonarCloud Scan
        uses: SonarSource/sonarcloud-github-action@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        with:
          args: >
            -Dsonar.organization=harini14hari
            -Dsonar.projectKey=harini14hari_Food-Rescue-Hub-
            -Dsonar.branch.name=main



  # =========================================================
  # STAGE 1: SCA + Secrets + Misconfig (Trivy FS)
  # (Uses official action -> avoids your gzip/tar failure)
  # =========================================================
  sca-trivy-fs:
    name: SCA + Secrets + Misconfig (Trivy FS)
    runs-on: ubuntu-latest
    needs: [detect]
    permissions:
      security-events: write
      contents: read
    steps:
      - uses: actions/checkout@v4

      - name: Trivy FS scan -> SARIF
        uses: aquasecurity/trivy-action@0.24.0
        with:
          scan-type: fs
          scan-ref: .
          format: sarif
          output: trivy-fs.sarif
          severity: HIGH,CRITICAL
          scanners: vuln,secret,config
          ignore-unfixed: true

      - name: Upload Trivy SARIF to Security tab
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: trivy-fs.sarif

      - name: Upload Trivy report artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: trivy-fs-sarif
          path: trivy-fs.sarif
          if-no-files-found: warn

      - name: Gate on HIGH/CRITICAL (optional)
        if: env.REPORT_ONLY != 'true'
        uses: aquasecurity/trivy-action@0.24.0
        with:
          scan-type: fs
          scan-ref: .
          format: table
          severity: HIGH,CRITICAL
          scanners: vuln,secret,config
          ignore-unfixed: true
          exit-code: "1"


  # =========================================================
  # STAGE 2: Integration Tests (Compose + Schemathesis + optional Playwright)
  # =========================================================
  integration-tests:
    name: Integration Tests (Compose + Schemathesis + optional Playwright)
    runs-on: ubuntu-latest
    needs:
      - detect
      - backend-junit
      - python-tests
      - lint-code-quality
      - sonarcloud
      - sca-trivy-fs
    if: >
      (github.event_name == 'push' && (github.ref == 'refs/heads/Sprint2' || github.ref == 'refs/heads/main')) ||
      (github.event_name == 'pull_request' && github.base_ref == 'main')
    steps:
      - uses: actions/checkout@v4

      - name: Skip (no compose file)
        if: needs.detect.outputs.has_compose != 'true'
        run: echo "No docker-compose.yml/compose.yml found -> skipping integration."

      - name: Start stack (Docker Compose)
        if: needs.detect.outputs.has_compose == 'true'
        shell: bash
        run: |
          set -e
          if [ -f "docker-compose.integration.yml" ]; then
            docker compose -f docker-compose.integration.yml up -d
            docker compose -f docker-compose.integration.yml ps
          elif [ -f "docker-compose.yml" ]; then
            docker compose -f docker-compose.yml up -d
            docker compose -f docker-compose.yml ps
          else
            docker compose -f compose.yml up -d
            docker compose -f compose.yml ps
          fi

      - name: Wait for MySQL (health)
        if: needs.detect.outputs.has_compose == 'true'
        env:
          MYSQL_ROOT_PASSWORD: ${{ secrets.MYSQL_ROOT_PASSWORD || 'password' }}
        shell: bash
        run: |
          set -e
          echo "Waiting for MySQL..."
          for i in {1..120}; do
            if docker compose -f docker-compose.integration.yml exec -T mysql mysqladmin ping -h localhost -uroot -p"${MYSQL_ROOT_PASSWORD}" --silent; then
              echo "MySQL is up!"
              exit 0
            fi
            echo "Waiting for MySQL... ($i)"
            sleep 2
          done
          echo "MySQL did not become ready in 120 seconds."
          docker compose -f docker-compose.integration.yml logs --no-color
          exit 1

      - name: Wait for backend (health)
        if: needs.detect.outputs.has_compose == 'true'
        shell: bash
        env:
          BACKEND_LOCAL_URL: "http://localhost:8080"
          SMOKE_PATH: "/api/hello"
        run: |
          set -e
          echo "Health check URL: ${BACKEND_LOCAL_URL}${SMOKE_PATH}"
          for i in {1..60}; do
            if curl -fsS --retry 3 --retry-delay 2 "${BACKEND_LOCAL_URL}${SMOKE_PATH}" >/dev/null 2>&1; then
              echo "Backend is up"
              exit 0
            fi
            echo "Waiting for backend... ($i)"
            sleep 5
          done
          echo "Backend did not become ready"
          if [ -f "docker-compose.integration.yml" ]; then
            docker compose -f docker-compose.integration.yml logs --no-color || true
          elif [ -f "docker-compose.yml" ]; then
            docker compose -f docker-compose.yml logs --no-color || true
          else
            docker compose -f compose.yml logs --no-color || true
          fi
          exit 1
      
      - name: Probe OpenAPI schema endpoints
        if: needs.detect.outputs.has_compose == 'true'
        shell: bash
        env:
          BACKEND_LOCAL_URL: "http://localhost:8080"
        run: |
          set +e
          echo "== Probing OpenAPI endpoints =="
          echo "GET ${BACKEND_LOCAL_URL}/v3/api-docs"
          curl -i "${BACKEND_LOCAL_URL}/v3/api-docs" | head -200
          echo
          echo "GET ${BACKEND_LOCAL_URL}/api/v3/api-docs"
          curl -i "${BACKEND_LOCAL_URL}/api/v3/api-docs" | head -200
          echo
          echo "GET ${BACKEND_LOCAL_URL}/swagger-ui/index.html"
          curl -i "${BACKEND_LOCAL_URL}/swagger-ui/index.html" | head -80

      - name: Backend logs (OpenAPI failure debug)
        if: needs.detect.outputs.has_compose == 'true'
        shell: bash
        run: |
          if [ -f "docker-compose.integration.yml" ]; then
            docker compose -f docker-compose.integration.yml logs --no-color --tail=300 || true
          elif [ -f "docker-compose.yml" ]; then
            docker compose -f docker-compose.yml logs --no-color --tail=300 || true
          else
            docker compose -f compose.yml logs --no-color --tail=300 || true
          fi

      # ---- Schemathesis from Swagger/OpenAPI ----
      - name: Setup Python (Schemathesis)
        if: needs.detect.outputs.has_compose == 'true'
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Schemathesis API tests (auto from Swagger)
        if: needs.detect.outputs.has_compose == 'true'
        shell: bash
        run: |
          set -euo pipefail
          pip install "schemathesis>=3,<4"

          mkdir -p schemathesis-out

          schemathesis --version | tee schemathesis-out/version.txt

          BASE_URL="${SCHEMATHESIS_BASE_URL:-${BACKEND_LOCAL_URL}}"
          PRIMARY_OPENAPI_URL="${BASE_URL}${OPENAPI_PATH}"
          ALT_OPENAPI_URL="${BASE_URL}/api${OPENAPI_PATH}"
          OPENAPI_URL=""

          echo "Schemathesis base URL: ${BASE_URL}"
          echo "OpenAPI candidate URLs:"
          echo " - ${PRIMARY_OPENAPI_URL}"
          echo " - ${ALT_OPENAPI_URL}"

          # Wait for OpenAPI to be reachable and valid JSON schema.
          for candidate in "${PRIMARY_OPENAPI_URL}" "${ALT_OPENAPI_URL}"; do
            for i in {1..20}; do
              HTTP_CODE="$(curl -sS -o schemathesis-out/openapi.json -w "%{http_code}" "${candidate}" || true)"
              if [ "${HTTP_CODE}" = "200" ] && grep -Eq '"(openapi|swagger)"[[:space:]]*:' schemathesis-out/openapi.json; then
                OPENAPI_URL="${candidate}"
                echo "OpenAPI schema fetched from ${OPENAPI_URL}"
                break 2
              fi
              echo "Waiting for OpenAPI schema at ${candidate}... (${i}/20, http=${HTTP_CODE})"
              sleep 3
            done
          done

          if [ -z "${OPENAPI_URL}" ]; then
            echo "Failed to download OpenAPI schema from ${PRIMARY_OPENAPI_URL} (also tried ${ALT_OPENAPI_URL})" | tee schemathesis-out/schemathesis.log

            {
              echo "== Probe: ${PRIMARY_OPENAPI_URL} =="
              curl -sS -i "${PRIMARY_OPENAPI_URL}" | head -n 80 || true
              echo
              echo "== Probe: ${ALT_OPENAPI_URL} =="
              curl -sS -i "${ALT_OPENAPI_URL}" | head -n 80 || true
              echo
              echo "== Probe: ${BACKEND_LOCAL_URL}${SMOKE_PATH} =="
              curl -sS -i "${BACKEND_LOCAL_URL}${SMOKE_PATH}" | head -n 40 || true
              echo
              echo "== Probe: ${BACKEND_LOCAL_URL}/actuator/health =="
              curl -sS -i "${BACKEND_LOCAL_URL}/actuator/health" | head -n 40 || true
            } >> schemathesis-out/schemathesis.log

            if [ -f "docker-compose.integration.yml" ]; then
              docker compose -f docker-compose.integration.yml logs --no-color backend >> schemathesis-out/schemathesis.log || true
            elif [ -f "docker-compose.yml" ]; then
              docker compose -f docker-compose.yml logs --no-color backend >> schemathesis-out/schemathesis.log || true
            else
              docker compose -f compose.yml logs --no-color backend >> schemathesis-out/schemathesis.log || true
            fi

            cat > schemathesis-junit.xml <<'EOF'
          <?xml version="1.0" encoding="UTF-8"?>
          <testsuites tests="1" failures="1" errors="0" skipped="0">
            <testsuite name="schemathesis" tests="1" failures="1" errors="0" skipped="0">
              <testcase name="schema-fetch">
                <failure message="Failed to fetch OpenAPI schema">OpenAPI endpoint was not reachable in CI. Checked primary and /api fallback paths.</failure>
              </testcase>
            </testsuite>
          </testsuites>
          EOF
            [ "${REPORT_ONLY}" = "true" ] && exit 0 || exit 1
          fi

          if schemathesis run --help | grep -q -- "--report-junit-path"; then
            REPORT_FLAGS=(--report junit --report-junit-path schemathesis-junit.xml)
          else
            REPORT_FLAGS=(--junit-xml schemathesis-junit.xml)
          fi

          # Schemathesis <4 requires an explicit experimental flag for OpenAPI 3.1 schemas.
          EXPERIMENT_FLAGS=()
          if grep -Eq '"openapi"[[:space:]]*:[[:space:]]*"3\.1(\.[0-9]+)?"' schemathesis-out/openapi.json; then
            echo "Detected OpenAPI 3.1 schema."
            if schemathesis run --help | grep -q -- "--experimental"; then
              EXPERIMENT_FLAGS=(--experimental=openapi-3.1)
              echo "Enabling Schemathesis experimental OpenAPI 3.1 support."
            fi
          fi

          # Schemathesis 3.x uses --base-url; keep backward compatibility if older CLI exposes --url.
          if schemathesis run --help | grep -q -- "--base-url"; then
            BASE_URL_FLAGS=(--base-url "${BASE_URL}")
          elif schemathesis run --help | grep -q -- "--url"; then
            BASE_URL_FLAGS=(--url "${BASE_URL}")
          else
            BASE_URL_FLAGS=()
          fi

          set +e
          schemathesis run schemathesis-out/openapi.json \
            "${BASE_URL_FLAGS[@]}" \
            "${EXPERIMENT_FLAGS[@]}" \
            --checks all \
            "${REPORT_FLAGS[@]}" 2>&1 | tee schemathesis-out/schemathesis.log
          ST_EXIT=$?
          set -e

          # Ensure artifact is never an empty file, even on CLI/runtime failures.
          if [ ! -s schemathesis-junit.xml ]; then
            cat > schemathesis-junit.xml <<EOF
          <?xml version="1.0" encoding="UTF-8"?>
          <testsuites tests="1" failures="1" errors="0" skipped="0">
            <testsuite name="schemathesis" tests="1" failures="1" errors="0" skipped="0">
              <testcase name="schemathesis-run">
                <failure message="Schemathesis did not generate JUnit XML">Exit code: ${ST_EXIT}. Check schemathesis-out/schemathesis.log.</failure>
              </testcase>
            </testsuite>
          </testsuites>
          EOF
          fi

          if [ "${ST_EXIT}" -ne 0 ] && [ "${REPORT_ONLY}" != "true" ]; then
            exit "${ST_EXIT}"
          fi

      - name: Upload Schemathesis report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: schemathesis-report
          path: |
            schemathesis-junit.xml
            schemathesis-out/**
          if-no-files-found: warn

      # ---- Playwright UI E2E (optional) ----
      - name: Setup Node 20 (Playwright)
        if: needs.detect.outputs.has_frontend == 'true' && needs.detect.outputs.has_playwright == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: npm
          cache-dependency-path: frontend/package-lock.json

      - name: Playwright E2E (optional)
        if: needs.detect.outputs.has_frontend == 'true' && needs.detect.outputs.has_playwright == 'true'
        working-directory: frontend
        shell: bash
        run: |
          set -e
          npm ci
          npx playwright install --with-deps
          npx playwright test || ( [ "${REPORT_ONLY}" = "true" ] && exit 0 || exit 1 )

      - name: Upload Playwright report (optional)
        if: always() && needs.detect.outputs.has_frontend == 'true' && needs.detect.outputs.has_playwright == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: playwright-report
          path: |
            frontend/playwright-report/**
            frontend/test-results/**
          if-no-files-found: warn

      - name: Stop stack
        if: always() && needs.detect.outputs.has_compose == 'true'
        shell: bash
        run: |
          if [ -f "docker-compose.integration.yml" ]; then
            docker compose -f docker-compose.integration.yml down -v
          elif [ -f "docker-compose.yml" ]; then
            docker compose -f docker-compose.yml down -v
          else
            docker compose -f compose.yml down -v
          fi


  # =========================================================
  # STAGE 2.4: IAST (OpenTelemetry-based, open-source)
  # =========================================================

  iast-opentelemetry:
    name: IAST (OpenTelemetry runtime â€“ open source)
    runs-on: ubuntu-latest
    needs: [detect, integration-tests]
    if: >
      github.event_name == 'push' &&
      github.ref == 'refs/heads/main' &&
      needs.detect.outputs.has_backend == 'true' &&
      needs.detect.outputs.has_compose == 'true'
    env:
      MYSQL_ROOT_PASSWORD: ${{ secrets.MYSQL_ROOT_PASSWORD || 'password' }}
      IAST_COMPOSE_FILE: docker-compose.integration.yml
    steps:
      - uses: actions/checkout@v4

      # Download OpenTelemetry Java agent (runner side)
      - name: Prepare OpenTelemetry agent
        run: |
          mkdir -p otel
          curl -L -o otel/opentelemetry-javaagent.jar \
            https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar

      # Start application with Docker Compose (agent mounted via volume)
      - name: Start application
        run: docker compose -f "${IAST_COMPOSE_FILE}" up -d

      # Wait for MySQL to be accepting connections after stack is started
      - name: Wait for MySQL
        shell: bash
        run: |
          set -e
          echo "Waiting for MySQL container to be ready..."
          for i in {1..60}; do
            if docker compose -f "${IAST_COMPOSE_FILE}" exec -T mysql mysqladmin ping -h localhost -uroot -p"${MYSQL_ROOT_PASSWORD}" >/dev/null 2>&1; then
              echo "MySQL is up"
              exit 0
            fi
            echo "Waiting for MySQL... ($i)"
            sleep 5
          done
          echo "MySQL did not become ready"
          docker compose -f "${IAST_COMPOSE_FILE}" logs --no-color || true
          exit 1

      # Wait for backend
      - name: Wait for backend
        run: |
          for i in {1..60}; do
            if curl -fsS "${BACKEND_LOCAL_URL}${SMOKE_PATH}" >/dev/null; then
              echo "Backend is up"
              exit 0
            fi
            echo "Waiting for backend... ($i)"
            sleep 5
          done
          docker compose -f "${IAST_COMPOSE_FILE}" logs
          exit 1

      # IAST-style attack probes (runtime execution)
      - name: Run IAST probes
        env:
          BACKEND_LOCAL_URL: ${{ env.BACKEND_LOCAL_URL }}
          SMOKE_PATH: ${{ env.SMOKE_PATH }}
        run: |
          set -euo pipefail
          echo "IAST base URL: [${BACKEND_LOCAL_URL}] [${SMOKE_PATH}] -> ${BACKEND_LOCAL_URL}${SMOKE_PATH}"
          if [ -z "${BACKEND_LOCAL_URL}" ]; then echo "BACKEND_LOCAL_URL not set"; exit 1; fi
          if [ -z "${SMOKE_PATH}" ]; then echo "SMOKE_PATH not set"; exit 1; fi

          base="${BACKEND_LOCAL_URL}${SMOKE_PATH}"

          # URL-encoded payloads to avoid spaces/quotes in raw URL
          p1="%27%20OR%20%271%27%3D%271%27"          # ' OR '1'='1
          p2="..%2F..%2Fetc%2Fpasswd"                  # ../../etc/passwd
          p3="%3Cscript%3Ealert(1)%3C%2Fscript%3E"    # <script>alert(1)</script>

          echo "Probing: ${base}?x=${p1}"
          curl --fail --show-error --silent -- "${base}?x=${p1}" || true

          echo "Probing: ${base}?file=${p2}"
          curl --fail --show-error --silent -- "${base}?file=${p2}" || true

          echo "Probing: ${base}?name=${p3}"
          curl --fail --show-error --silent -- "${base}?name=${p3}" || true

      # Collect runtime logs
      - name: Collect runtime logs
        if: always()
        run: |
          mkdir -p iast-out
          docker compose -f "${IAST_COMPOSE_FILE}" logs --no-color > iast-out/runtime.log

      # Detect runtime security indicators (real signals)
      - name: Detect runtime security findings
        run: |
          if grep -Ei \
            "SQLSyntaxErrorException|SQLException.*' OR|HibernateException|FileNotFoundException.*\.\./|Whitelabel Error Page|path traversal|directory traversal|NullPointerException|ClassCastException.*XSS|eval\(|<script>" \
            iast-out/runtime.log; then
            echo "IAST runtime issue detected"
            exit 1
          fi
          echo "No runtime security indicators detected"

      - name: Upload IAST evidence
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: iast-opentelemetry-report
          path: iast-out

      - name: Stop stack
        if: always()
        run: docker compose -f "${IAST_COMPOSE_FILE}" down -v


  # =========================================================
  # STAGE 2.5: Selenium Functional Pre-Validation
  # =========================================================
  selenium-functional-test:
    name: Selenium Functional Test (Pre-Load Validation)
    runs-on: ubuntu-latest
    needs: [detect, integration-tests, iast-opentelemetry]
    if: >
      always() &&
      needs.detect.outputs.has_backend == 'true' &&
      needs.integration-tests.result == 'success' &&
      (
        github.ref != 'refs/heads/main' ||
        needs.iast-opentelemetry.result == 'success'
      )
    env:
      STAGING_URL: "http://localhost:80"
      BACKEND_LOCAL_URL: "http://localhost:8080"
    steps:
      - uses: actions/checkout@v4

      - name: Set up JDK 17
        uses: actions/setup-java@v4
        with:
          java-version: "17"
          distribution: "temurin"
          cache: maven

      - name: Set up ChromeDriver
        uses: nanasess/setup-chromedriver@v2

      - name: Start application stack
        env:
          MYSQL_ROOT_PASSWORD: ${{ secrets.MYSQL_ROOT_PASSWORD || 'password' }}
        run: |
          if [ -f "docker-compose.integration.yml" ]; then
            docker compose -f docker-compose.integration.yml up -d
          else
            docker compose -f docker-compose.yml up -d
          fi

      - name: Wait for MySQL
        env:
          MYSQL_ROOT_PASSWORD: ${{ secrets.MYSQL_ROOT_PASSWORD || 'password' }}
        run: |
          for i in {1..120}; do
            if docker compose -f docker-compose.integration.yml exec -T mysql mysqladmin ping -h localhost -uroot -p"${MYSQL_ROOT_PASSWORD}" --silent; then
              echo "MySQL is ready"
              exit 0
            fi
            echo "Waiting for MySQL... ($i)"
            sleep 2
          done
          echo "MySQL failed to start"
          docker compose -f docker-compose.integration.yml logs
          exit 1

      - name: Wait for backend
        run: |
          for i in {1..60}; do
            if curl -fsS "${BACKEND_LOCAL_URL}${SMOKE_PATH}" >/dev/null; then
              echo "Backend is ready"
              exit 0
            fi
            echo "Waiting for backend... ($i)"
            sleep 5
          done
          docker compose -f docker-compose.integration.yml logs
          exit 1

      - name: Run Selenium functional tests
        working-directory: backend
        env:
          STAGING_URL: ${{ env.STAGING_URL }}
          BACKEND_LOCAL_URL: ${{ env.BACKEND_LOCAL_URL }}
        run: |
          chmod +x mvnw
          ./mvnw -q test -Dtest=BackendApplicationTests -Dsurefire.failIfNoSpecifiedTests=false

      - name: Upload Selenium test report
        if: always() && hashFiles('backend/target/surefire-reports/*') != ''
        uses: actions/upload-artifact@v4
        with:
          name: selenium-functional-test-report
          path: backend/target/surefire-reports/**
          if-no-files-found: warn

      - name: Tear down stack
        if: always()
        run: |
          if [ -f "docker-compose.integration.yml" ]; then
            docker compose -f docker-compose.integration.yml down -v
          elif [ -f "docker-compose.yml" ]; then
            docker compose -f docker-compose.yml down -v
          else
            docker compose -f compose.yml down -v
          fi


  # =========================================================
  # STAGE 2.6: JMeter Load & Performance Testing
  # =========================================================
  jmeter-load-performance-test:
    name: Load & Performance Testing (JMeter)
    runs-on: ubuntu-latest
    needs: [detect, selenium-functional-test]
    if: needs.detect.outputs.has_backend == 'true'
    env:
      WF_JMETER_TARGET_HOST: ${{ inputs.jmeter_target_host }}
      WF_JMETER_TARGET_PORT: ${{ inputs.jmeter_target_port }}
      WF_JMETER_TARGET_PROTOCOL: ${{ inputs.jmeter_target_protocol }}
      VAR_JMETER_TARGET_HOST: ${{ vars.JMETER_TARGET_HOST }}
      VAR_JMETER_TARGET_PORT: ${{ vars.JMETER_TARGET_PORT }}
      VAR_JMETER_TARGET_PROTOCOL: ${{ vars.JMETER_TARGET_PROTOCOL }}
      JMETER_VERSION: "5.6.3"
      JMETER_WARMUP_USERS: "8"
      JMETER_WARMUP_LOOPS: "3"
      JMETER_WARMUP_RAMP_SECONDS: "10"
      JMETER_LOAD_USERS: "20"
      JMETER_LOAD_LOOPS: "5"
      JMETER_LOAD_RAMP_SECONDS: "60"
      JMETER_THINK_TIME_MS: "350"
      JMETER_CONNECT_TIMEOUT_MS: "10000"
      JMETER_RESPONSE_TIMEOUT_MS: "10000"
      JMETER_MAX_RESPONSE_MS: "5000"
      JMETER_LAT: "1.3521"
      JMETER_LNG: "103.8198"
      JMETER_RADIUS_KM: "5.0"
      METRICS_SAMPLE_SECONDS: "5"
      EC2_HOST: ${{ secrets.EC2_HOST }}
      EC2_SSH_KEY: ${{ secrets.EC2_SSH_KEY }}
    steps:
      - uses: actions/checkout@v4

      - name: Resolve JMeter target
        id: jmeter_target
        shell: bash
        run: |
          set -euo pipefail

          TARGET_HOST="${WF_JMETER_TARGET_HOST:-${VAR_JMETER_TARGET_HOST:-localhost}}"
          TARGET_PORT="${WF_JMETER_TARGET_PORT:-${VAR_JMETER_TARGET_PORT:-8080}}"
          TARGET_PROTOCOL="${WF_JMETER_TARGET_PROTOCOL:-${VAR_JMETER_TARGET_PROTOCOL:-http}}"
          RUN_MODE="deployed"
          if [ "${TARGET_HOST}" = "localhost" ] || [ "${TARGET_HOST}" = "127.0.0.1" ]; then
            RUN_MODE="local"
          fi
          TARGET_URL="${TARGET_PROTOCOL}://${TARGET_HOST}:${TARGET_PORT}"

          {
            echo "target_host=${TARGET_HOST}"
            echo "target_port=${TARGET_PORT}"
            echo "target_protocol=${TARGET_PROTOCOL}"
            echo "target_url=${TARGET_URL}"
            echo "run_mode=${RUN_MODE}"
          } >> "${GITHUB_OUTPUT}"

          {
            echo "run_mode=${RUN_MODE}"
            echo "target_url=${TARGET_URL}"
            echo "target_host=${TARGET_HOST}"
            echo "target_port=${TARGET_PORT}"
            echo "target_protocol=${TARGET_PROTOCOL}"
            echo "resolved_at_utc=$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          } > jmeter-target.txt

          cat jmeter-target.txt

      - name: Install JMeter
        run: |
          echo "Installing Apache JMeter ${JMETER_VERSION}..."
          wget -q "https://dlcdn.apache.org/jmeter/binaries/apache-jmeter-${JMETER_VERSION}.tgz"
          tar -xzf "apache-jmeter-${JMETER_VERSION}.tgz"
          echo "JMeter installed successfully"
          ./apache-jmeter-${JMETER_VERSION}/bin/jmeter --version

      - name: Start application stack
        id: local_stack
        if: steps.jmeter_target.outputs.run_mode == 'local'
        env:
          MYSQL_ROOT_PASSWORD: ${{ secrets.MYSQL_ROOT_PASSWORD || 'password' }}
        run: |
          if [ -f "docker-compose.integration.yml" ]; then
            COMPOSE_FILE="docker-compose.integration.yml"
          elif [ -f "docker-compose.yml" ]; then
            COMPOSE_FILE="docker-compose.yml"
          else
            COMPOSE_FILE="compose.yml"
          fi
          echo "compose_file=${COMPOSE_FILE}" >> "${GITHUB_OUTPUT}"
          docker compose -f "${COMPOSE_FILE}" up -d

      - name: Wait for MySQL
        if: steps.jmeter_target.outputs.run_mode == 'local'
        env:
          MYSQL_ROOT_PASSWORD: ${{ secrets.MYSQL_ROOT_PASSWORD || 'password' }}
        run: |
          COMPOSE_FILE="${{ steps.local_stack.outputs.compose_file }}"
          for i in {1..120}; do
            if docker compose -f "${COMPOSE_FILE}" exec -T mysql mysqladmin ping -h localhost -uroot -p"${MYSQL_ROOT_PASSWORD}" --silent; then
              echo "MySQL is ready"
              exit 0
            fi
            echo "Waiting for MySQL... ($i)"
            sleep 2
          done
          echo "MySQL failed to start"
          docker compose -f "${COMPOSE_FILE}" logs
          exit 1

      - name: Wait for local backend
        if: steps.jmeter_target.outputs.run_mode == 'local'
        run: |
          for i in {1..60}; do
            if curl -fsS "${{ steps.jmeter_target.outputs.target_url }}/api/hello" >/dev/null; then
              echo "Backend is ready for load testing"
              exit 0
            fi
            echo "Waiting for backend... ($i)"
            sleep 5
          done
          docker compose logs
          exit 1

      - name: Wait for deployed backend
        if: steps.jmeter_target.outputs.run_mode == 'deployed'
        shell: bash
        run: |
          TARGET_URL="${{ steps.jmeter_target.outputs.target_url }}"
          for i in {1..30}; do
            if curl -fsS "${TARGET_URL}/api/hello" >/dev/null; then
              echo "Deployed backend is reachable at ${TARGET_URL}"
              exit 0
            fi
            echo "Waiting for deployed backend... ($i)"
            sleep 5
          done
          echo "Deployed target did not become ready: ${TARGET_URL}"
          exit 1

      - name: Start local metrics sampling
        if: steps.jmeter_target.outputs.run_mode == 'local'
        shell: bash
        run: |
          set -euo pipefail
          {
            echo "# local docker stats sampling"
            echo "# started_utc=$(date -u +%Y-%m-%dT%H:%M:%SZ)"
            echo "timestamp_utc,container_name,cpu_perc,mem_perc,mem_usage,net_io,block_io,pids"
          } > system-metrics-local.log

          (
            while true; do
              TS="$(date -u +%Y-%m-%dT%H:%M:%SZ)"
              docker stats --no-stream --format "{{.Name}},{{.CPUPerc}},{{.MemPerc}},{{.MemUsage}},{{.NetIO}},{{.BlockIO}},{{.PIDs}}" \
                | sed "s/^/${TS},/" >> system-metrics-local.log
              sleep "${METRICS_SAMPLE_SECONDS}"
            done
          ) &
          echo $! > .metrics_local_pid

      - name: Start remote metrics sampling
        if: steps.jmeter_target.outputs.run_mode == 'deployed'
        shell: bash
        run: |
          set -euo pipefail
          TARGET_HOST="${{ steps.jmeter_target.outputs.target_host }}"

          if [ -z "${EC2_HOST}" ] || [ -z "${EC2_SSH_KEY}" ] || [ "${TARGET_HOST}" != "${EC2_HOST}" ]; then
            echo "Skipping remote metrics: requires EC2_HOST/EC2_SSH_KEY and target host == EC2_HOST" | tee system-metrics-remote.log
            exit 0
          fi

          KEYFILE="${RUNNER_TEMP}/ec2_jmeter_metrics_key"
          printf '%s' "${EC2_SSH_KEY}" > "${KEYFILE}"
          chmod 600 "${KEYFILE}"
          echo "remote_keyfile=${KEYFILE}" > .remote_metrics_ctx
          echo "remote_host=${TARGET_HOST}" >> .remote_metrics_ctx
          echo "started_utc=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> .remote_metrics_ctx

          ssh -o BatchMode=yes -o StrictHostKeyChecking=no -i "${KEYFILE}" ubuntu@"${TARGET_HOST}" \
            "nohup bash -lc 'echo \"# remote docker stats sampling\" > /tmp/frh-jmeter-remote-metrics.log; \
             echo \"# started_utc=\$(date -u +%Y-%m-%dT%H:%M:%SZ)\" >> /tmp/frh-jmeter-remote-metrics.log; \
             while true; do \
               echo \"--- \$(date -u +%Y-%m-%dT%H:%M:%SZ) ---\" >> /tmp/frh-jmeter-remote-metrics.log; \
               docker stats --no-stream >> /tmp/frh-jmeter-remote-metrics.log 2>&1; \
               sleep ${METRICS_SAMPLE_SECONDS}; \
             done' >/dev/null 2>&1 & echo \$! > /tmp/frh-jmeter-remote-metrics.pid"

      - name: Run JMeter Load Test (Non-GUI Mode)
        run: |
          echo "==================================================="
          echo "Starting JMeter Load & Performance Test"
          echo "==================================================="
          echo "Test Configuration:"
          echo "  - Warm-up: ${JMETER_WARMUP_USERS} users x ${JMETER_WARMUP_LOOPS} loops (ramp ${JMETER_WARMUP_RAMP_SECONDS}s)"
          echo "  - Main load: ${JMETER_LOAD_USERS} users x ${JMETER_LOAD_LOOPS} loops (ramp ${JMETER_LOAD_RAMP_SECONDS}s)"
          echo "  - Weighted mix: 70% /api/listings, 20% /api/listings/category/All, 10% /api/listings/nearby"
          echo "  - Max response time assertion: ${JMETER_MAX_RESPONSE_MS}ms"
          echo "  - Target: ${{ steps.jmeter_target.outputs.target_url }}"
          echo "==================================================="
          
          ./apache-jmeter-${JMETER_VERSION}/bin/jmeter \
            -n \
            -t tests/jmeter/load-test.jmx \
            -l results.jtl \
            -e \
            -o report \
            -JHOST="${{ steps.jmeter_target.outputs.target_host }}" \
            -JPORT="${{ steps.jmeter_target.outputs.target_port }}" \
            -JPROTOCOL="${{ steps.jmeter_target.outputs.target_protocol }}" \
            -JWARMUP_USERS="${JMETER_WARMUP_USERS}" \
            -JWARMUP_LOOPS="${JMETER_WARMUP_LOOPS}" \
            -JWARMUP_RAMP_SECONDS="${JMETER_WARMUP_RAMP_SECONDS}" \
            -JLOAD_USERS="${JMETER_LOAD_USERS}" \
            -JLOAD_LOOPS="${JMETER_LOAD_LOOPS}" \
            -JLOAD_RAMP_SECONDS="${JMETER_LOAD_RAMP_SECONDS}" \
            -JTHINK_TIME_MS="${JMETER_THINK_TIME_MS}" \
            -JCONNECT_TIMEOUT_MS="${JMETER_CONNECT_TIMEOUT_MS}" \
            -JRESPONSE_TIMEOUT_MS="${JMETER_RESPONSE_TIMEOUT_MS}" \
            -JMAX_RESPONSE_MS="${JMETER_MAX_RESPONSE_MS}" \
            -JLAT="${JMETER_LAT}" \
            -JLNG="${JMETER_LNG}" \
            -JRADIUS_KM="${JMETER_RADIUS_KM}"
          
          echo "==================================================="
          echo "JMeter Load Test Completed"
          echo "==================================================="

      - name: Stop local metrics sampling
        if: always() && steps.jmeter_target.outputs.run_mode == 'local'
        shell: bash
        run: |
          if [ -f ".metrics_local_pid" ]; then
            kill "$(cat .metrics_local_pid)" || true
          fi

      - name: Stop and collect remote metrics
        if: always() && steps.jmeter_target.outputs.run_mode == 'deployed'
        shell: bash
        run: |
          set +e
          if [ ! -f ".remote_metrics_ctx" ]; then
            echo "No remote metrics context found." | tee -a system-metrics-remote.log
            exit 0
          fi

          KEYFILE="$(grep '^remote_keyfile=' .remote_metrics_ctx | cut -d= -f2-)"
          TARGET_HOST="$(grep '^remote_host=' .remote_metrics_ctx | cut -d= -f2-)"

          ssh -o BatchMode=yes -o StrictHostKeyChecking=no -i "${KEYFILE}" ubuntu@"${TARGET_HOST}" \
            "if [ -f /tmp/frh-jmeter-remote-metrics.pid ]; then kill \$(cat /tmp/frh-jmeter-remote-metrics.pid) 2>/dev/null || true; fi"
          ssh -o BatchMode=yes -o StrictHostKeyChecking=no -i "${KEYFILE}" ubuntu@"${TARGET_HOST}" \
            "cat /tmp/frh-jmeter-remote-metrics.log 2>/dev/null || echo 'No remote metrics log found'" \
            > system-metrics-remote.log
          rm -f "${KEYFILE}" .remote_metrics_ctx

      - name: Display JMeter Summary
        if: always()
        run: |
          if [ -f "results.jtl" ]; then
            echo "=========================================="
            echo "JMETER LOAD TEST SUMMARY"
            echo "=========================================="
            echo ""
            echo "Total Samples:"
            wc -l < results.jtl
            echo ""
            echo "Error Rate:"
            if grep -q "false" results.jtl; then
              TOTAL=$(wc -l < results.jtl)
              ERRORS=$(grep -c "false" results.jtl || echo 0)
              echo "${ERRORS} errors out of ${TOTAL} requests"
              echo "Error rate: $(awk "BEGIN {printf \"%.2f%%\", ($ERRORS/$TOTAL)*100}")"
            else
              echo "0 errors"
            fi
            echo ""
            echo "Full HTML report generated in: report/index.html"
            echo "=========================================="
          fi

      - name: Upload JMeter Results (Raw Data)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: jmeter-results-jtl
          path: results.jtl
          if-no-files-found: warn

      - name: Upload JMeter Performance Report (HTML)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: jmeter-performance-report
          path: report/
          if-no-files-found: warn

      - name: Upload Runtime Metrics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: jmeter-runtime-metrics
          path: |
            jmeter-target.txt
            system-metrics-local.log
            system-metrics-remote.log
          if-no-files-found: warn

      - name: Archive Load Test Evidence
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: load-test-complete-evidence
          path: |
            jmeter-target.txt
            results.jtl
            report/**
            system-metrics-local.log
            system-metrics-remote.log
            tests/jmeter/load-test.jmx
          if-no-files-found: warn

      - name: Tear down stack
        if: always()
        run: |
          if [ "${{ steps.jmeter_target.outputs.run_mode }}" != "local" ]; then
            echo "No local stack teardown needed for deployed target mode."
            exit 0
          fi
          COMPOSE_FILE="${{ steps.local_stack.outputs.compose_file }}"
          docker compose -f "${COMPOSE_FILE}" down -v

  # =========================================================
  # STAGE 3: Build & Push Docker Images (main only)
  # =========================================================
  build-and-push:
    name: Build & Push Docker Images (main)
    runs-on: ubuntu-latest
    needs: [detect, integration-tests, jmeter-load-performance-test]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    env:
      DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
      DOCKER_PASSWORD: ${{ secrets.DOCKER_PASSWORD }}
    steps:
      - uses: actions/checkout@v4
      - uses: docker/setup-buildx-action@v3

      - name: Skip if Docker credentials missing
        shell: bash
        run: |
          if [ -z "${DOCKER_USERNAME}" ] || [ -z "${DOCKER_PASSWORD}" ]; then
            echo "DOCKER_USERNAME/DOCKER_PASSWORD missing -> skipping build & push"
            exit 0
          fi

      - name: Login to Docker Hub
        if: env.DOCKER_USERNAME != '' && env.DOCKER_PASSWORD != ''
        uses: docker/login-action@v3
        with:
          username: ${{ env.DOCKER_USERNAME }}
          password: ${{ env.DOCKER_PASSWORD }}

      - name: Build & push backend image
        if: needs.detect.outputs.has_backend == 'true' && env.DOCKER_USERNAME != '' && env.DOCKER_PASSWORD != ''
        shell: bash
        run: |
          set -e
          docker build -t "${DOCKER_USERNAME}/frh-backend:latest" -f backend/Dockerfile backend
          docker push "${DOCKER_USERNAME}/frh-backend:latest"

      - name: Build & push frontend image
        if: needs.detect.outputs.has_frontend == 'true' && env.DOCKER_USERNAME != '' && env.DOCKER_PASSWORD != ''
        shell: bash
        env:
          VITE_GOOGLE_MAPS_API_KEY: ${{ secrets.VITE_GOOGLE_MAPS_API_KEY }}
        run: |
          set -e
          docker build --build-arg VITE_GOOGLE_MAPS_API_KEY="${VITE_GOOGLE_MAPS_API_KEY}" -t "${DOCKER_USERNAME}/frh-frontend:latest" -f frontend/Dockerfile .
          docker push "${DOCKER_USERNAME}/frh-frontend:latest"


  # =========================================================
  # STAGE 3: Container Image Scan (Trivy Image) - main only
  # =========================================================
  image-scan-trivy:
    name: Container Image Scan (Trivy) - main
    runs-on: ubuntu-latest
    needs: [detect, build-and-push]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    env:
      DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
      DOCKER_PASSWORD: ${{ secrets.DOCKER_PASSWORD }}
      TRIVY_TIMEOUT: 30m
    steps:
      - name: Skip if Docker credentials missing
        shell: bash
        run: |
          if [ -z "${DOCKER_USERNAME}" ] || [ -z "${DOCKER_PASSWORD}" ]; then
            echo "DOCKER_USERNAME/DOCKER_PASSWORD missing -> skipping image scan"
            exit 0
          fi

      - name: Login to Docker Hub
        if: env.DOCKER_USERNAME != '' && env.DOCKER_PASSWORD != ''
        uses: docker/login-action@v3
        with:
          username: ${{ env.DOCKER_USERNAME }}
          password: ${{ env.DOCKER_PASSWORD }}

      - name: Pull backend image (if exists)
        if: needs.detect.outputs.has_backend == 'true' && env.DOCKER_USERNAME != '' && env.DOCKER_PASSWORD != ''
        shell: bash
        run: |
          set -e
          docker pull "${DOCKER_USERNAME}/frh-backend:latest"
          docker save "${DOCKER_USERNAME}/frh-backend:latest" -o "${GITHUB_WORKSPACE}/frh-backend-image.tar"

      - name: Pull frontend image (if exists)
        if: needs.detect.outputs.has_frontend == 'true' && env.DOCKER_USERNAME != '' && env.DOCKER_PASSWORD != ''
        shell: bash
        run: |
          set -e
          docker pull "${DOCKER_USERNAME}/frh-frontend:latest"
          docker save "${DOCKER_USERNAME}/frh-frontend:latest" -o "${GITHUB_WORKSPACE}/frh-frontend-image.tar"

      - name: Scan backend image (if exists)
        if: needs.detect.outputs.has_backend == 'true' && env.DOCKER_USERNAME != ''
        uses: aquasecurity/trivy-action@0.24.0
        with:
          input: frh-backend-image.tar
          scan-type: image
          format: table
          exit-code: "0"
          severity: "CRITICAL,HIGH"
          scanners: vuln

      - name: Scan frontend image (if exists)
        if: needs.detect.outputs.has_frontend == 'true' && env.DOCKER_USERNAME != ''
        uses: aquasecurity/trivy-action@0.24.0
        with:
          input: frh-frontend-image.tar
          scan-type: image
          format: table
          exit-code: "0"
          severity: "CRITICAL,HIGH"
          scanners: vuln


  # =========================================================
  # STAGE 3: Performance (k6) - optional (main only)
  # Fix: heredoc EOF MUST NOT be indented.
  # =========================================================
  perf-k6:
    name: Performance (k6) - optional
    runs-on: ubuntu-latest
    needs: [iast-opentelemetry, image-scan-trivy]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    env:
      STAGING_URL: ${{ secrets.STAGING_URL }}
    steps:
      - uses: actions/checkout@v4

      - name: Resolve k6 target URL
        id: k6_target
        shell: bash
        run: |
          set -euo pipefail
          RAW="${STAGING_URL:-}"

          if [ -z "$(printf '%s' "${RAW}" | tr -d '[:space:]')" ] || [ "${RAW}" = "***" ]; then
            echo "should_run=false" >> "${GITHUB_OUTPUT}"
            echo "reason=missing_or_masked_staging_url" >> "${GITHUB_OUTPUT}"
            echo "k6 skipped: STAGING_URL is missing or masked."
            exit 0
          fi

          TARGET="${RAW%/}"
          if [[ ! "${TARGET}" =~ ^https?://[^[:space:]/]+(:[0-9]+)?(/.*)?$ ]]; then
            echo "should_run=false" >> "${GITHUB_OUTPUT}"
            echo "reason=invalid_staging_url" >> "${GITHUB_OUTPUT}"
            echo "k6 skipped: invalid STAGING_URL format. Expected http(s)://host[:port][/optional-path]"
            exit 0
          fi

          echo "should_run=true" >> "${GITHUB_OUTPUT}"
          echo "reason=ready" >> "${GITHUB_OUTPUT}"
          echo "target=${TARGET}" >> "${GITHUB_OUTPUT}"
          echo "k6 target resolved: ${TARGET}"

      - name: Print k6 skip reason
        if: steps.k6_target.outputs.should_run != 'true'
        shell: bash
        run: |
          echo "k6 stage skipped (${{ steps.k6_target.outputs.reason }}). Set secret STAGING_URL to enable this optional stage."

      - name: Probe k6 target URL
        id: k6_probe
        if: steps.k6_target.outputs.should_run == 'true'
        shell: bash
        run: |
          set -euo pipefail
          TARGET="${{ steps.k6_target.outputs.target }}"
          echo "k6 target: ${TARGET}"
          if curl -fsS --connect-timeout 5 --max-time 15 --retry 2 --retry-connrefused "${TARGET}/api/hello" >/dev/null; then
            echo "should_run=true" >> "${GITHUB_OUTPUT}"
            echo "reason=reachable" >> "${GITHUB_OUTPUT}"
          else
            echo "should_run=false" >> "${GITHUB_OUTPUT}"
            echo "reason=unreachable_target" >> "${GITHUB_OUTPUT}"
            echo "k6 skipped: target is unreachable from this GitHub runner."
          fi

      - name: Print k6 probe skip reason
        if: steps.k6_target.outputs.should_run == 'true' && steps.k6_probe.outputs.should_run != 'true'
        shell: bash
        run: |
          echo "k6 stage skipped (${{ steps.k6_probe.outputs.reason }}). Ensure STAGING_URL is reachable from GitHub-hosted runners."

      - name: Install k6
        if: steps.k6_target.outputs.should_run == 'true' && steps.k6_probe.outputs.should_run == 'true'
        shell: bash
        run: |
          set -e
          sudo apt-get update
          sudo apt-get install -y gnupg ca-certificates curl
          curl -s https://dl.k6.io/key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/k6-archive-keyring.gpg
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install -y k6

      - name: Run k6 (HTTP smoke + thresholds)
        if: steps.k6_target.outputs.should_run == 'true' && steps.k6_probe.outputs.should_run == 'true'
        shell: bash
        run: |
          set -euo pipefail
          if [ ! -f "tests/k6/script.js" ]; then
            echo "Missing tests/k6/script.js"
            exit 1
          fi

          TARGET="${{ steps.k6_target.outputs.target }}"
          echo "Running k6 against ${TARGET}"

          if [ "${REPORT_ONLY}" = "true" ]; then
            k6 run \
              --summary-export=k6-summary.json \
              -e BASE_URL="${TARGET}" \
              -e STAGING_URL="${TARGET}" \
              tests/k6/script.js 2>&1 | tee k6-run.log || true
          else
            k6 run \
              --summary-export=k6-summary.json \
              -e BASE_URL="${TARGET}" \
              -e STAGING_URL="${TARGET}" \
              tests/k6/script.js 2>&1 | tee k6-run.log
          fi

      - name: Upload k6 report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: k6-report
          path: |
            k6-summary.json
            k6-run.log
            tests/k6/script.js
          if-no-files-found: warn



  # =========================================================
  # STAGE 3: DAST (OWASP ZAP) - optional (main only)
  # =========================================================
  dast-zap:
    name: DAST (OWASP ZAP) - optional
    runs-on: ubuntu-latest
    needs: [perf-k6]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    env:
      STAGING_URL: ${{ secrets.STAGING_URL }}
    steps:
      - name: Skip if STAGING_URL not set
        shell: bash
        run: |
          if [ -z "${STAGING_URL}" ]; then
            echo "STAGING_URL not set -> skipping ZAP"
            exit 0
          fi

      - name: Run ZAP baseline scan
        if: env.STAGING_URL != ''
        shell: bash
        run: |
          set -e
          mkdir -p zap-out
          docker pull ghcr.io/zaproxy/zaproxy:stable

          docker run --rm \
            --user 0:0 \
            -v "$PWD/zap-out:/zap/wrk:rw" \
            ghcr.io/zaproxy/zaproxy:stable \
            zap-baseline.py \
              -t "${STAGING_URL}" \
              -J report_json.json \
              -w report_md.md \
              -r report_html.html \
              -a | tee zap-out/zap-console.log || true

          echo "ZAP output files:"
          ls -lah zap-out || true

      - name: Upload ZAP report
        if: always() && env.STAGING_URL != ''
        uses: actions/upload-artifact@v4
        with:
          name: zap-dast-report
          path: zap-out/
          if-no-files-found: warn


  # =========================================================
  # STAGE 4: Deploy to AWS EC2 (main only)
  # =========================================================
  deploy:
    name: Deploy to AWS EC2 (docker compose)
    runs-on: ubuntu-latest
    needs: [dast-zap]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    env:
      EC2_HOST: ${{ secrets.EC2_HOST }}
      EC2_SSH_KEY: ${{ secrets.EC2_SSH_KEY }}
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      STRIPE_SECRET_KEY: ${{ secrets.STRIPE_SECRET_KEY }}
      PROD_URL: ${{ secrets.PROD_URL }}     # optional for smoke test
    steps:
      - name: Skip if EC2 secrets missing
        shell: bash
        run: |
          if [ -z "${EC2_HOST}" ] || [ -z "${EC2_SSH_KEY}" ]; then
            echo "EC2_HOST / EC2_SSH_KEY missing -> skipping deploy"
            exit 0
          fi

      - name: Check SSH connectivity
        shell: bash
        run: |
          set -euo pipefail

          # Write SSH key to a temporary file and restrict permissions
          KEYFILE="${RUNNER_TEMP}/ec2_deploy_key"
          echo "Writing SSH key to ${KEYFILE}"
          printf '%s' "${EC2_SSH_KEY}" > "${KEYFILE}"
          chmod 600 "${KEYFILE}"

          # Preflight SSH check with retries to provide clearer failure messages
          ATTEMPTS=3
          SLEEP=5
          for i in $(seq 1 ${ATTEMPTS}); do
            echo "Preflight SSH attempt ${i}/${ATTEMPTS}..."
            ssh -o BatchMode=yes -o StrictHostKeyChecking=no -i "${KEYFILE}" -o ConnectTimeout=10 ubuntu@"${EC2_HOST}" "echo connected" && FOUND=1 || FOUND=0
            if [ "${FOUND}" -eq 1 ]; then
              echo "SSH preflight succeeded"
              rm -f "${KEYFILE}"
              exit 0
            fi
            echo "Preflight SSH failed, sleeping ${SLEEP}s"
            sleep ${SLEEP}
            SLEEP=$((SLEEP * 2))
          done

          echo "SSH preflight failed after ${ATTEMPTS} attempts"
          echo "Check: EC2 instance running, Security Group allows TCP/22 from GitHub Actions or your IP, and the correct key is in GitHub secrets."
          # dump basic network diagnostic (non-sensitive)
          echo "Runner public IP fetch (for SG allowlist):"
          curl -s https://ifconfig.me || true
          rm -f "${KEYFILE}"
          exit 1

      - name: Resolve deploy smoke target
        id: deploy_target
        shell: bash
        run: |
          set -euo pipefail

          RAW_URL="$(printf '%s' "${PROD_URL:-}" | tr -d '[:space:]')"
          if [ -z "${RAW_URL}" ] || [ "${RAW_URL}" = "***" ]; then
            RAW_URL="http://${EC2_HOST}"
          fi

          TARGET_URL="${RAW_URL%/}"
          if [ -n "${TARGET_URL}" ] && [[ ! "${TARGET_URL}" =~ ^https?:// ]]; then
            TARGET_URL="http://${TARGET_URL}"
          fi

          echo "target_url=${TARGET_URL}" >> "${GITHUB_OUTPUT}"
          echo "Resolved deploy smoke target: ${TARGET_URL}"

      - name: Deploy via SSH
        if: env.EC2_HOST != '' && env.EC2_SSH_KEY != ''
        uses: appleboy/ssh-action@v1
        with:
          host: ${{ env.EC2_HOST }}
          username: ubuntu
          key: ${{ env.EC2_SSH_KEY }}
          envs: OPENAI_API_KEY,STRIPE_SECRET_KEY
          script: |
            set -euo pipefail
            cd ~/Food-Rescue-Hub-

            if [ -z "${OPENAI_API_KEY:-}" ]; then
              echo "OPENAI_API_KEY is missing; ai_service may fail to start"
            fi
            if [ -z "${STRIPE_SECRET_KEY:-}" ]; then
              echo "STRIPE_SECRET_KEY is missing; checkout payments will return 503"
            fi

            retry_cmd() {
              max_attempts="$1"
              shift
              attempt=1
              delay=3
              while true; do
                if "$@"; then
                  return 0
                fi
                rc=$?
                if [ "${attempt}" -ge "${max_attempts}" ]; then
                  echo "Command failed after ${attempt} attempts (exit ${rc}): $*"
                  return "${rc}"
                fi
                echo "Attempt ${attempt}/${max_attempts} failed (exit ${rc}): $*"
                echo "Retrying in ${delay}s..."
                sleep "${delay}"
                delay=$((delay * 2))
                attempt=$((attempt + 1))
              done
            }

            if [ -d .git ]; then
              if ! git remote | grep -qx "origin"; then
                echo "Git remote 'origin' is not configured in ~/Food-Rescue-Hub-"
                exit 1
              fi
              echo "Using origin: $(git remote get-url origin)"
              retry_cmd 5 git fetch --prune origin main
              # Recover from interrupted merge/rebase/cherry-pick/am states from prior failed deploys.
              git merge --abort >/dev/null 2>&1 || true
              git rebase --abort >/dev/null 2>&1 || true
              git cherry-pick --abort >/dev/null 2>&1 || true
              git am --abort >/dev/null 2>&1 || true
              if [ -n "$(git diff --name-only --diff-filter=U)" ]; then
                echo "EC2 repo has unresolved conflicts; discarding local git state before sync"
              fi
              if [ -n "$(git status --porcelain)" ]; then
                echo "EC2 repo has local changes; forcing clean sync with origin/main"
                git reset --hard HEAD
              fi
              git checkout -B main FETCH_HEAD
              git reset --hard FETCH_HEAD
            else
              echo "Directory ~/Food-Rescue-Hub- is not a git repository"
              exit 1
            fi

            if [ -f docker-compose.yml ]; then
              docker compose -f docker-compose.yml config >/dev/null
              if ! docker compose -f docker-compose.yml config --services | grep -qx 'mysql'; then
                echo "docker-compose.yml on EC2 does not contain required service: mysql"
                docker compose -f docker-compose.yml config --services || true
                exit 1
              fi
              docker compose -f docker-compose.yml pull
              docker compose -f docker-compose.yml up -d
              docker compose -f docker-compose.yml ps
            elif [ -f compose.yml ]; then
              docker compose -f compose.yml config >/dev/null
              if ! docker compose -f compose.yml config --services | grep -qx 'mysql'; then
                echo "compose.yml on EC2 does not contain required service: mysql"
                docker compose -f compose.yml config --services || true
                exit 1
              fi
              docker compose -f compose.yml pull
              docker compose -f compose.yml up -d
              docker compose -f compose.yml ps
            else
              echo "No compose file on server in ~/Food-Rescue-Hub-"
              exit 1
            fi

            docker system prune -f

      - name: Post-deploy smoke test (wait & verify)
        shell: bash
        env:
          TARGET_URL: ${{ steps.deploy_target.outputs.target_url }}
        run: |
          set -euo pipefail

          if [ -z "${TARGET_URL}" ]; then
            echo "Target URL not resolved -> skipping smoke test"
            exit 0
          fi

          declare -a CANDIDATES=("${TARGET_URL}")
          if [ -n "${EC2_HOST}" ]; then
            CANDIDATES+=("http://${EC2_HOST}" "http://${EC2_HOST}:8080")
          fi

          declare -a TARGETS=()
          for candidate in "${CANDIDATES[@]}"; do
            candidate="${candidate%/}"
            duplicate=0
            for existing in "${TARGETS[@]}"; do
              if [ "${existing}" = "${candidate}" ]; then
                duplicate=1
                break
              fi
            done
            if [ "${duplicate}" -eq 0 ]; then
              TARGETS+=("${candidate}")
            fi
          done

          echo "Smoke targets:"
          for base in "${TARGETS[@]}"; do
            echo " - ${base}${SMOKE_PATH}"
          done

          for i in {1..30}; do
            for base in "${TARGETS[@]}"; do
              if curl -fsS --connect-timeout 5 --max-time 15 --retry 1 --retry-connrefused "${base}${SMOKE_PATH}" >/dev/null; then
                echo "App is up at ${base}${SMOKE_PATH}"
                exit 0
              fi
            done
            echo "Waiting for app... ($i)"
            sleep 5
          done

          echo "App never became ready from the GitHub runner."
          echo "Check secret PROD_URL, EC2 Security Group ingress (TCP 80/8080), and container status on the host."
          exit 1

      - name: Verify app on EC2 localhost
        if: always() && env.EC2_HOST != '' && env.EC2_SSH_KEY != ''
        uses: appleboy/ssh-action@v1
        with:
          host: ${{ env.EC2_HOST }}
          username: ubuntu
          key: ${{ env.EC2_SSH_KEY }}
          script: |
            set -e
            cd ~/Food-Rescue-Hub-
            SMOKE_PATH="${{ env.SMOKE_PATH }}"
            if [ -z "${SMOKE_PATH}" ]; then
              SMOKE_PATH="/api/hello"
            fi
            case "${SMOKE_PATH}" in
              /*) ;;
              *) SMOKE_PATH="/${SMOKE_PATH}" ;;
            esac
            echo "Using local SMOKE_PATH=${SMOKE_PATH}"

            if [ -f docker-compose.yml ]; then
              COMPOSE_FILE="docker-compose.yml"
            elif [ -f compose.yml ]; then
              COMPOSE_FILE="compose.yml"
            else
              echo "No compose file on server in ~/Food-Rescue-Hub-"
              exit 1
            fi

            for i in {1..30}; do
              if curl -fsS --connect-timeout 3 --max-time 10 "http://127.0.0.1${SMOKE_PATH}" >/dev/null; then
                echo "EC2 localhost smoke passed via nginx"
                exit 0
              fi
              if curl -fsS --connect-timeout 3 --max-time 10 "http://127.0.0.1:8080${SMOKE_PATH}" >/dev/null; then
                echo "EC2 localhost smoke passed via backend"
                exit 0
              fi
              echo "Waiting for local app on EC2... ($i)"
              sleep 5
            done

            docker compose -f "${COMPOSE_FILE}" ps || true
            docker compose -f "${COMPOSE_FILE}" logs --tail=80 nginx backend frontend mysql || true
            exit 1

      - name: Remote diagnostics on smoke failure
        if: failure() && env.EC2_HOST != '' && env.EC2_SSH_KEY != ''
        uses: appleboy/ssh-action@v1
        with:
          host: ${{ env.EC2_HOST }}
          username: ubuntu
          key: ${{ env.EC2_SSH_KEY }}
          script: |
            set +e
            cd ~/Food-Rescue-Hub-
            SMOKE_PATH="${{ env.SMOKE_PATH }}"
            if [ -z "${SMOKE_PATH}" ]; then
              SMOKE_PATH="/api/hello"
            fi
            case "${SMOKE_PATH}" in
              /*) ;;
              *) SMOKE_PATH="/${SMOKE_PATH}" ;;
            esac
            echo "Using local SMOKE_PATH=${SMOKE_PATH}"

            if [ -f docker-compose.yml ]; then
              COMPOSE_FILE="docker-compose.yml"
            elif [ -f compose.yml ]; then
              COMPOSE_FILE="compose.yml"
            else
              echo "No compose file on server in ~/Food-Rescue-Hub-"
              exit 0
            fi

            echo "== docker compose ps =="
            docker compose -f "${COMPOSE_FILE}" ps || true

            echo "== localhost probe via nginx =="
            curl -sS -i --connect-timeout 3 --max-time 10 "http://127.0.0.1${SMOKE_PATH}" | head -n 30 || true

            echo "== localhost probe direct backend =="
            curl -sS -i --connect-timeout 3 --max-time 10 "http://127.0.0.1:8080${SMOKE_PATH}" | head -n 30 || true

            echo "== recent logs (nginx/backend/frontend/mysql) =="
            docker compose -f "${COMPOSE_FILE}" logs --tail=80 nginx backend frontend mysql || true
  ci-metrics-summary:
      name: CI Metrics Summary (JSON)
      runs-on: ubuntu-latest
      needs:
        - backend-junit
        - python-tests
        - lint-code-quality
      if: always()

      steps:
      - name: Create CI metrics JSON
        shell: bash
        run: |
          mkdir -p ci-metrics

          cat <<EOF > ci-metrics/ci-metrics.json
          {
            "repository": "${{ github.repository }}",
            "workflow": "${{ github.workflow }}",
            "run_id": "${{ github.run_id }}",
            "run_number": "${{ github.run_number }}",
            "branch": "${{ github.ref_name }}",
            "commit": "${{ github.sha }}",
            "triggered_by": "${{ github.actor }}",
            "pipeline_status": "${{ job.status }}",
            "timestamp_utc": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          }
          EOF

      - name: Upload CI metrics artifact
        uses: actions/upload-artifact@v4
        with:
          name: ci-metrics
          path: ci-metrics/ci-metrics.json

